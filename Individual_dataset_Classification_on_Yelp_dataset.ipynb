{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Becky0214/COMP8240-Major-Project-Individual-Dataset-Yelp/blob/main/Individual_dataset_Classification_on_Yelp_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVYYofwVR6R2"
      },
      "outputs": [],
      "source": [
        "#Install necessary packages\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets\n",
        "#install the dataset"
      ],
      "metadata": {
        "id": "mmvtQ_VGSUMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQB2bdkwSWS9",
        "outputId": "d08ba64d-8af5-4af8-fc6f-6f96ccfedaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C-LSTM FINE-GRAINED CLASSIFICATTION ON THE YELP DATASET**"
      ],
      "metadata": {
        "id": "qnsCISArf_-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and preprocess the Yelp dataset\n",
        "dataset = load_dataset(\"yelp_review_full\", split=\"train\")\n",
        "\n",
        "# Extract reviews and ratings\n",
        "texts = dataset['text']\n",
        "ratings = dataset['label']  # Labels are integers from 0 to 4\n",
        "\n",
        "# Convert ratings to binary labels (e.g., 4–5 stars = positive (1), 1–3 stars = negative (0))\n",
        "binary_labels = [1 if rating >= 3 else 0 for rating in ratings]\n",
        "\n",
        "# Tokenization and padding\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 300\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_data = pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "y_data = np.array(binary_labels)\n",
        "\n",
        "# Split data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path, embedding_dim=300):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding_vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = embedding_vector\n",
        "    return embeddings_index\n",
        "\n",
        "# Create embedding matrix\n",
        "def create_embedding_matrix(word_index, glove_embeddings, vocab_size, embedding_dim=300):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size:\n",
        "            embedding_vector = glove_embeddings.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load pre-trained GloVe embeddings (update this path)\n",
        "glove_file_path = \"/content/drive/MyDrive/glove.6B.300d.txt\"\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
        "embedding_matrix = create_embedding_matrix(tokenizer.word_index, glove_embeddings, VOCAB_SIZE)\n",
        "\n",
        "# Step 3: Define the C-LSTM Model for Binary Classification\n",
        "class CLSTMBinaryClassifierYelp(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_matrix, max_length, num_filters=150, lstm_units=150, embedding_dim=300, dropout_rate=0.5, l2_reg_lambda=0.001):\n",
        "        super(CLSTMBinaryClassifierYelp, self).__init__()\n",
        "        self.embedding = layers.Embedding(input_dim=vocab_size,\n",
        "                                          output_dim=embedding_dim,\n",
        "                                          input_length=max_length,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          trainable=True)\n",
        "        self.embedding_dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        # Convolutional layer with filter size 3\n",
        "        self.conv_layer = layers.Conv2D(filters=num_filters,\n",
        "                                        kernel_size=(3, embedding_dim),\n",
        "                                        activation='relu', padding='valid')\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "        # LSTM layer to capture dependencies\n",
        "        self.lstm = layers.LSTM(lstm_units, return_sequences=False)\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        # Output layer for binary classification\n",
        "        self.fc = layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L2(l2_reg_lambda))\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.embedding_dropout(x, training=training)\n",
        "        x = tf.expand_dims(x, -1)\n",
        "\n",
        "        # Apply convolutional layer and batch normalization\n",
        "        conv_out = self.conv_layer(x)\n",
        "        conv_out = self.batch_norm(conv_out, training=training)\n",
        "        conv_out = tf.squeeze(conv_out, 2)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        rnn_outputs = self.lstm(conv_out)\n",
        "        rnn_outputs = self.dropout(rnn_outputs, training=training)\n",
        "\n",
        "        # Output for binary classification\n",
        "        binary_output = self.fc(rnn_outputs)\n",
        "        return binary_output\n",
        "\n",
        "# Step 4: Initialize and compile the model\n",
        "model = CLSTMBinaryClassifierYelp(vocab_size=VOCAB_SIZE,\n",
        "                              embedding_matrix=embedding_matrix,\n",
        "                              max_length=MAX_LEN)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaVM8Tz1eZKC",
        "outputId": "ee1a3a4a-4e6e-4bda-f67b-377b988f461f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 24ms/step - accuracy: 0.8322 - loss: 0.3679 - val_accuracy: 0.8928 - val_loss: 0.2503\n",
            "Epoch 2/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 24ms/step - accuracy: 0.8907 - loss: 0.2593 - val_accuracy: 0.8938 - val_loss: 0.2480\n",
            "Epoch 3/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 24ms/step - accuracy: 0.8963 - loss: 0.2478 - val_accuracy: 0.8918 - val_loss: 0.2575\n",
            "Epoch 4/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 24ms/step - accuracy: 0.9001 - loss: 0.2411 - val_accuracy: 0.9018 - val_loss: 0.2360\n",
            "Epoch 5/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 24ms/step - accuracy: 0.9031 - loss: 0.2343 - val_accuracy: 0.9029 - val_loss: 0.2315\n",
            "Epoch 6/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 23ms/step - accuracy: 0.9038 - loss: 0.2310 - val_accuracy: 0.9039 - val_loss: 0.2334\n",
            "Epoch 7/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 24ms/step - accuracy: 0.9056 - loss: 0.2284 - val_accuracy: 0.9044 - val_loss: 0.2279\n",
            "Epoch 8/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 24ms/step - accuracy: 0.9082 - loss: 0.2248 - val_accuracy: 0.9033 - val_loss: 0.2310\n",
            "Epoch 9/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 24ms/step - accuracy: 0.9083 - loss: 0.2241 - val_accuracy: 0.9056 - val_loss: 0.2267\n",
            "Epoch 10/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 24ms/step - accuracy: 0.9101 - loss: 0.2206 - val_accuracy: 0.9041 - val_loss: 0.2291\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.9049 - loss: 0.2281\n",
            "Test Loss: 0.2290487438440323, Test Accuracy: 0.904130756855011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C-LSTM FINE-GRAINED CLASSIFICATTION ON THE YELP DATASET**"
      ],
      "metadata": {
        "id": "fg8lpoCYDNL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and preprocess the Yelp dataset\n",
        "dataset = load_dataset(\"yelp_review_full\", split=\"train\")\n",
        "\n",
        "# Extract reviews and ratings\n",
        "texts = dataset['text']\n",
        "fine_grained_labels = dataset['label']  # Labels are integers from 1 to 5\n",
        "\n",
        "# Tokenization and padding\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 300\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_data = pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "y_data = np.array(fine_grained_labels)\n",
        "\n",
        "# Split data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path, embedding_dim=300):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding_vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = embedding_vector\n",
        "    return embeddings_index\n",
        "\n",
        "# Create embedding matrix\n",
        "def create_embedding_matrix(word_index, glove_embeddings, vocab_size, embedding_dim=300):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size:\n",
        "            embedding_vector = glove_embeddings.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load pre-trained GloVe embeddings (update this path)\n",
        "glove_file_path = \"/content/drive/MyDrive/glove.6B.300d.txt\"\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
        "embedding_matrix = create_embedding_matrix(tokenizer.word_index, glove_embeddings, VOCAB_SIZE)\n",
        "\n",
        "# Step 3: Define the C-LSTM Model for Fine-Grained Classification (5 classes)\n",
        "class CLSTMFineGrainedClassifierYelp(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_matrix, max_length, num_filters=150, lstm_units=150, num_classes=5, embedding_dim=300, dropout_rate=0.5, l2_reg_lambda=0.001):\n",
        "        super(CLSTMFineGrainedClassifierYelp, self).__init__()\n",
        "        self.embedding = layers.Embedding(input_dim=vocab_size,\n",
        "                                          output_dim=embedding_dim,\n",
        "                                          input_length=max_length,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          trainable=True)\n",
        "        self.embedding_dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        # Convolutional layer with filter size 3\n",
        "        self.conv_layer = layers.Conv2D(filters=num_filters,\n",
        "                                        kernel_size=(3, embedding_dim),\n",
        "                                        activation='relu', padding='valid')\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "        # LSTM layer to capture dependencies\n",
        "        self.lstm = layers.LSTM(lstm_units, return_sequences=False)\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        # Output layer for fine-grained classification\n",
        "        self.fc = layers.Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(l2_reg_lambda))\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.embedding_dropout(x, training=training)\n",
        "        x = tf.expand_dims(x, -1)\n",
        "\n",
        "        # Apply convolutional layer and batch normalization\n",
        "        conv_out = self.conv_layer(x)\n",
        "        conv_out = self.batch_norm(conv_out, training=training)\n",
        "        conv_out = tf.squeeze(conv_out, 2)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        rnn_outputs = self.lstm(conv_out)\n",
        "        rnn_outputs = self.dropout(rnn_outputs, training=training)\n",
        "\n",
        "        # Output for fine-grained classification\n",
        "        multi_class_output = self.fc(rnn_outputs)\n",
        "        return multi_class_output\n",
        "\n",
        "# Step 4: Initialize and compile the model\n",
        "model = CLSTMFineGrainedClassifierYelp(vocab_size=VOCAB_SIZE,\n",
        "                                  embedding_matrix=embedding_matrix,\n",
        "                                  max_length=MAX_LEN,\n",
        "                                  num_classes=5)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLNTwJGJCw3x",
        "outputId": "ac0714f8-4c18-4d6c-a0dc-66c108f6afa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 24ms/step - accuracy: 0.5242 - loss: 1.0898 - val_accuracy: 0.6268 - val_loss: 0.8569\n",
            "Epoch 2/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6252 - loss: 0.8667 - val_accuracy: 0.6445 - val_loss: 0.8207\n",
            "Epoch 3/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6411 - loss: 0.8351 - val_accuracy: 0.6503 - val_loss: 0.8155\n",
            "Epoch 4/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6494 - loss: 0.8186 - val_accuracy: 0.6518 - val_loss: 0.8090\n",
            "Epoch 5/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6540 - loss: 0.8067 - val_accuracy: 0.6554 - val_loss: 0.8000\n",
            "Epoch 6/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6573 - loss: 0.7999 - val_accuracy: 0.6575 - val_loss: 0.7926\n",
            "Epoch 7/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6602 - loss: 0.7932 - val_accuracy: 0.6583 - val_loss: 0.7902\n",
            "Epoch 8/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6646 - loss: 0.7877 - val_accuracy: 0.6611 - val_loss: 0.7859\n",
            "Epoch 9/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6674 - loss: 0.7800 - val_accuracy: 0.6592 - val_loss: 0.7887\n",
            "Epoch 10/10\n",
            "\u001b[1m8125/8125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 23ms/step - accuracy: 0.6686 - loss: 0.7760 - val_accuracy: 0.6606 - val_loss: 0.7880\n",
            "\u001b[1m4063/4063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.6592 - loss: 0.7897\n",
            "Test Accuracy: 0.6605538725852966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iM0AhzisCxfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}